{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络\n",
    "## tf.nn.conv2d\n",
    "[TF-卷积函数 tf.nn.conv2d 介绍](https://www.cnblogs.com/qggg/p/6832342.html)\n",
    "````python\n",
    "tf.nn.conv2d(input, # [batch, in_height, in_width, in_channels]\n",
    "            filter, # [filter_height, filter_width, in_channels, out_channels]\n",
    "            strides, # [1,1,1,1]\n",
    "            padding, # SAME VALID\n",
    "            use_cudnn_on_gpu=True,\n",
    "            data_format='NHWC',\n",
    "            dilations=[1, 1, 1, 1],\n",
    "            name=None)\n",
    "````\n",
    "- input: [batch, in_height, in_width, in_channels] 4-D的 Tensor (float32/float64)\n",
    "       batch_size,高度，宽度，通道数\n",
    "- filter：[filter_height, filter_width, in_channels, out_channels]\n",
    "        卷积核高度，   卷积核宽度      输入通道数    输出通道数(卷积核个数)\n",
    "- strides：步长 [1,1,1,1] 1-D向量，长度为4\n",
    "- padding：填充 SAME VALID\n",
    "- use_cudnn_on_gpu：是否GPU加速\n",
    "\n",
    "结果返回一个Tensor，这个输出，就是我们常说的 feature map，shape仍然是 [batch, height, width, channels] 这种形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像卷积\n",
    "#### 3x3图像，1通道，1x1卷积核\n",
    "1.考虑一种最简单的情况，现在有一张3×3单通道的图像（对应的shape：[1，3，3，1]），用一个1×1的卷积核（对应的shape：[1，1，1，1]）去做卷积，最后会得到一张3×3的feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.06386475]\n",
      "   [ 0.30251193]\n",
      "   [-0.36254457]]\n",
      "\n",
      "  [[-0.1863834 ]\n",
      "   [-0.11046342]\n",
      "   [ 0.12128225]]\n",
      "\n",
      "  [[-0.16598591]\n",
      "   [-0.06247617]\n",
      "   [ 0.10344568]]]] \n",
      "\n",
      "(1, 3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([1,3,3,1]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([1,1,1,1]))\n",
    "op1 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op1),'\\n')\n",
    "    print(sess.run(op1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3x3图像，5通道，1x1卷积核\n",
    "2.增加图片的通道数，使用一张3×3五通道的图像（对应的shape：[1，3，3，5]），用一个1×1的卷积核（对应的shape：[1，1，1，1]）去做卷积，仍然是一张3×3的feature map，这就相当于每一个像素点，卷积核都与该像素点的每一个通道做卷积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 2.611188 ]\n",
      "   [ 1.1356436]\n",
      "   [ 0.1728566]]\n",
      "\n",
      "  [[ 1.7238054]\n",
      "   [-2.65044  ]\n",
      "   [-0.6026321]]\n",
      "\n",
      "  [[-0.9376406]\n",
      "   [-2.1341398]\n",
      "   [ 0.5801886]]]] \n",
      "\n",
      "(1, 3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([1,3,3,5]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([1,1,5,1]))\n",
    "op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op2),'\\n')\n",
    "    print(sess.run(op2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3x3图像，5通道，3x3卷积核，文本卷积维度和图像一样大，所以卷积之后只有1列\n",
    "3.把卷积核扩大，现在用3×3的卷积核做卷积，最后的输出是一个值，相当于情况2的feature map所有像素点的值求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-3.8528938]]]] \n",
      "\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([1,3,3,5]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,1]))\n",
    "op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op2),'\\n')\n",
    "    print(sess.run(op2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5x5图像，5通道，3x3卷积核\n",
    "4.使用更大的图片将情况2的图片扩大到5×5，仍然是3×3的卷积核，令步长为1，输出3×3的feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-3.3537188]\n",
      "   [ 2.6631894]\n",
      "   [10.7735815]]\n",
      "\n",
      "  [[ 6.7866626]\n",
      "   [-5.753437 ]\n",
      "   [ 6.8379397]]\n",
      "\n",
      "  [[-7.2338777]\n",
      "   [-3.8412943]\n",
      "   [11.663807 ]]]] \n",
      "\n",
      "(1, 3, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,1]))\n",
    "op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op2),'\\n')\n",
    "    print(sess.run(op2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5x5图像，5通道，3x3卷积核\n",
    "5.上面我们一直令参数padding的值为‘VALID’，当其为‘SAME’时，表示卷积核可以停留在图像边缘，如下，输出5×5的feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ -4.1736193 ]\n",
      "   [  7.2922435 ]\n",
      "   [  2.9188058 ]\n",
      "   [  0.49713266]\n",
      "   [ -2.956664  ]]\n",
      "\n",
      "  [[ -3.305164  ]\n",
      "   [ -7.311406  ]\n",
      "   [ -5.045771  ]\n",
      "   [ -2.5354984 ]\n",
      "   [ -7.40237   ]]\n",
      "\n",
      "  [[  9.273168  ]\n",
      "   [  1.2130424 ]\n",
      "   [ -8.63011   ]\n",
      "   [  8.675023  ]\n",
      "   [  4.0911283 ]]\n",
      "\n",
      "  [[ -2.295607  ]\n",
      "   [  5.2230077 ]\n",
      "   [-10.142306  ]\n",
      "   [ -6.135029  ]\n",
      "   [  1.3315554 ]]\n",
      "\n",
      "  [[-11.159186  ]\n",
      "   [ -3.5029335 ]\n",
      "   [ -1.638276  ]\n",
      "   [ -4.381499  ]\n",
      "   [ -1.0199151 ]]]] \n",
      "\n",
      "(1, 5, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,1]))\n",
    "op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op2),'\\n')\n",
    "    print(sess.run(op2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5x5图像，5通道，3x3卷积核，3个卷积核\n",
    "6.如果卷积核有多个\n",
    "\n",
    "此时输出3张5×5的feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  2.7570508   -3.232238    -2.6215773 ]\n",
      "   [  2.532285     1.9889098    3.87929   ]\n",
      "   [ -3.187311     8.91769      3.224719  ]\n",
      "   [  0.44387245  -9.403946     2.468867  ]\n",
      "   [  0.21311586  -1.590601     5.749056  ]]\n",
      "\n",
      "  [[ -2.6277003    6.488189     9.992645  ]\n",
      "   [  1.5766766  -11.48576      0.6145782 ]\n",
      "   [ -5.0482545   -0.96584886   4.0381684 ]\n",
      "   [ -0.797274     2.4302173   -3.8855307 ]\n",
      "   [ -2.6238062    2.05465      2.9259453 ]]\n",
      "\n",
      "  [[  4.714437    -0.9536078   -2.9879472 ]\n",
      "   [  1.5400691    1.5240853   -6.90153   ]\n",
      "   [ -3.6736727    3.85059     -0.5918405 ]\n",
      "   [  7.023252     2.9593654  -13.595696  ]\n",
      "   [ -5.041815    -2.7133517    0.6385279 ]]\n",
      "\n",
      "  [[  1.477376     0.47209492   5.653083  ]\n",
      "   [ -0.39575818  14.780628    -1.5949147 ]\n",
      "   [  2.378466   -11.533363    -0.4041656 ]\n",
      "   [ -0.4129743    6.5807753   -2.7889323 ]\n",
      "   [  6.1631317   -0.49479347   1.52246   ]]\n",
      "\n",
      "  [[ -6.97586      1.1432166   -5.064254  ]\n",
      "   [ -9.823753    -3.1042528   -1.6604922 ]\n",
      "   [  9.015108   -10.42481     -4.7503257 ]\n",
      "   [  2.3552632    0.43692362   2.7325256 ]\n",
      "   [ -2.1840062    2.729301    -4.588225  ]]]] \n",
      "\n",
      "(1, 5, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,3]))\n",
    "op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op2),'\\n')\n",
    "    print(sess.run(op2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5x5图像，5通道，3x3卷积核，3个卷积核，步长为2\n",
    "7.步长不为1的情况，文档里说了对于图片，因为只有两维，通常strides取[1，stride，stride，1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 2.783487  -2.09441    4.733526 ]\n",
      "   [-7.3059773 -3.108855   4.022243 ]\n",
      "   [-4.050215   3.0158758 -4.1893964]]\n",
      "\n",
      "  [[-6.3690815 -5.2265515  1.1703218]\n",
      "   [ 9.0784235 -3.5745146 14.855592 ]\n",
      "   [-0.4078823  4.0576644  4.617129 ]]\n",
      "\n",
      "  [[ 3.339266  -5.4302483 -3.154387 ]\n",
      "   [ 1.3765206  2.6518223  5.6584387]\n",
      "   [-3.9308991  1.4282804 -3.4455342]]]] \n",
      "\n",
      "(1, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([1,5,5,5]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,3]))\n",
    "op2 = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op2),'\\n')\n",
    "    print(sess.run(op2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5x5图像，5通道，3x3卷积核，3个卷积核，步长为2，10张图像\n",
    "8.如果batch值不为1，同时输入10张图\n",
    "\n",
    "每张图，都有3张3×3的feature map，输出的shape就是[10，3，3，3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  3.7294517   -3.107039    -3.3261778 ]\n",
      "   [  5.441018    -4.0686336    5.619485  ]\n",
      "   [  2.1390946    3.5674727   -1.5291997 ]]\n",
      "\n",
      "  [[ -5.6689725   16.974398     2.381929  ]\n",
      "   [  2.4008992   -0.6694184    1.378117  ]\n",
      "   [  1.2934582    7.2192235    0.48349503]]\n",
      "\n",
      "  [[  2.9121394   -0.4573097   -9.765212  ]\n",
      "   [  1.0088806   -0.7046843    6.591536  ]\n",
      "   [ -0.72504395   0.43721557  -4.999654  ]]]\n",
      "\n",
      "\n",
      " [[[  1.967625    -1.6568589   -6.145099  ]\n",
      "   [ -4.151078   -10.529405    -2.047928  ]\n",
      "   [  3.4548922    2.7491624    2.9001775 ]]\n",
      "\n",
      "  [[ -1.6896939    3.1873543    6.188783  ]\n",
      "   [ 11.703161     1.6971766   -4.8438787 ]\n",
      "   [  0.9549799   -1.1131762    4.593415  ]]\n",
      "\n",
      "  [[ -1.1170579    0.4810401   -1.3526723 ]\n",
      "   [  2.529728    -1.1482326    3.7958796 ]\n",
      "   [ -0.24976896   3.3091352   -6.729189  ]]]\n",
      "\n",
      "\n",
      " [[[ -3.33531     -1.9344107    2.8165019 ]\n",
      "   [ -1.6785766   -2.8081656    7.2197647 ]\n",
      "   [  1.7976431    2.8334517   -0.08083367]]\n",
      "\n",
      "  [[  2.5362453   -0.68693405   2.2952533 ]\n",
      "   [  1.5236933    2.129165     0.194734  ]\n",
      "   [  0.5964938   -8.6989565    5.084363  ]]\n",
      "\n",
      "  [[  5.066878    -1.3026551   -5.7902007 ]\n",
      "   [ -2.9802423    4.8924155    5.9025197 ]\n",
      "   [ -3.933334     5.099715    -0.8536027 ]]]\n",
      "\n",
      "\n",
      " [[[ -1.0145748  -10.15126     -7.0179715 ]\n",
      "   [ -8.451802    -0.17334843  -2.7171214 ]\n",
      "   [  5.668031    -5.15528     -4.2402534 ]]\n",
      "\n",
      "  [[ -2.0954626   -1.0145442    3.2066696 ]\n",
      "   [  2.289553     0.9271075    0.8146973 ]\n",
      "   [ -0.7423492    4.2153864   -4.70488   ]]\n",
      "\n",
      "  [[  3.8675358   -0.35446188  -1.1588985 ]\n",
      "   [ -4.8492827   -5.2945166    3.944246  ]\n",
      "   [ -0.43092388  -0.8130417    2.3813803 ]]]\n",
      "\n",
      "\n",
      " [[[  0.66720986   3.8808417    2.1328838 ]\n",
      "   [  7.446735    -4.522188    -5.990181  ]\n",
      "   [ -2.5916054    5.0853543    2.6371577 ]]\n",
      "\n",
      "  [[ -7.136207     0.6306949   -8.853178  ]\n",
      "   [  3.7415988   -7.89348     -8.487032  ]\n",
      "   [ -0.69531     -3.222552     1.5073893 ]]\n",
      "\n",
      "  [[ -3.106504    -0.01809834  -9.029028  ]\n",
      "   [ -4.416857     0.13292897   7.7073345 ]\n",
      "   [  0.9844466    4.2795186   -0.76342046]]]\n",
      "\n",
      "\n",
      " [[[ -0.11672409  -7.369146    -4.8543487 ]\n",
      "   [ -4.230579    -1.1736143    0.74828875]\n",
      "   [ -0.6568188    6.765464    -4.9761944 ]]\n",
      "\n",
      "  [[  3.8933635   -7.902747    -0.63001007]\n",
      "   [  6.8245344    3.9199047   11.168122  ]\n",
      "   [  3.7043867    0.31197003   0.04769279]]\n",
      "\n",
      "  [[ -4.1409       8.580945     8.486864  ]\n",
      "   [ -3.1867335    7.059393     6.296857  ]\n",
      "   [ -0.9835455   -3.6718185    0.97860974]]]\n",
      "\n",
      "\n",
      " [[[  5.193179    -1.2967495    8.170371  ]\n",
      "   [ -1.3087153   -5.5033283   -1.6919953 ]\n",
      "   [  0.36510244  -6.296658    -3.7380807 ]]\n",
      "\n",
      "  [[  3.5434737   13.3447695    1.7701437 ]\n",
      "   [-10.250333    -9.407058    -3.2337494 ]\n",
      "   [  8.435421     4.078936     3.4657378 ]]\n",
      "\n",
      "  [[  3.4681797    2.228949     0.45596147]\n",
      "   [ -0.57005715   4.670751     2.034872  ]\n",
      "   [ -1.915133     7.9970365   -3.8922138 ]]]\n",
      "\n",
      "\n",
      " [[[  3.950432    -4.4767623   -6.447672  ]\n",
      "   [  2.595737    -8.553671    -0.45686972]\n",
      "   [  3.391854    -2.003466    -2.2928245 ]]\n",
      "\n",
      "  [[ -3.6888146    3.5153918    1.2406276 ]\n",
      "   [ -0.25753272   2.6999128   -2.8501456 ]\n",
      "   [ -0.9058769    6.502099    -0.5419939 ]]\n",
      "\n",
      "  [[ -0.68687534  -6.5038085    2.8593688 ]\n",
      "   [ -3.683316     2.1430447    5.490655  ]\n",
      "   [  5.7413816    3.3227494   -7.533464  ]]]\n",
      "\n",
      "\n",
      " [[[ -3.484571     5.3650527    2.9336984 ]\n",
      "   [  0.6027174    3.7776787    1.0154141 ]\n",
      "   [ -4.7919264    7.149525     1.9800262 ]]\n",
      "\n",
      "  [[ -2.1547816   -3.2360375   -5.2381744 ]\n",
      "   [  7.6362724    8.085188     9.068025  ]\n",
      "   [ -4.549206    -3.8285804    5.8914824 ]]\n",
      "\n",
      "  [[ -1.9079026    2.9233663    0.9151974 ]\n",
      "   [  6.70253    -10.376949    -2.2334673 ]\n",
      "   [  2.7263498    3.202616     3.6564238 ]]]\n",
      "\n",
      "\n",
      " [[[  6.6487746   -0.2954742    3.0371974 ]\n",
      "   [  3.576       -7.2807136    4.2893467 ]\n",
      "   [ -0.96813136  -5.533345    -6.83936   ]]\n",
      "\n",
      "  [[ -0.10136782  -1.4625425   -7.0081096 ]\n",
      "   [  3.8160882   -2.4150543   -3.9401052 ]\n",
      "   [  2.7480733   -1.4603323   10.289123  ]]\n",
      "\n",
      "  [[  2.629776    -3.5297518    0.4979372 ]\n",
      "   [  0.9985927   -8.139794    -0.5185237 ]\n",
      "   [ -4.5744176  -10.06965      5.6358476 ]]]] \n",
      "\n",
      "(10, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([10,5,5,5]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,3,5,3]))\n",
    "op2 = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op2),'\\n')\n",
    "    print(sess.run(op2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本卷积\n",
    "#### 5x5文本，1通道，2-gram卷积核，3个卷积核，10句话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ -0.28560075   3.4164069    0.64287925]]\n",
      "\n",
      "  [[  4.922906     6.569628    -4.377281  ]]\n",
      "\n",
      "  [[  2.0096815   -0.9498653   -5.302     ]]]\n",
      "\n",
      "\n",
      " [[[ -0.30866227   0.65657634   0.08617933]]\n",
      "\n",
      "  [[  2.1648352    2.0540233   -6.2501183 ]]\n",
      "\n",
      "  [[  1.8437229   -3.3579445    0.648278  ]]]\n",
      "\n",
      "\n",
      " [[[ -6.248692    -8.374758     3.1102016 ]]\n",
      "\n",
      "  [[ -4.6158714    1.0821313    2.8032086 ]]\n",
      "\n",
      "  [[  1.912092     0.933113    -3.0924444 ]]]\n",
      "\n",
      "\n",
      " [[[  7.5399313    9.936766    -1.6083889 ]]\n",
      "\n",
      "  [[  2.4991071   -0.3938322    5.2363515 ]]\n",
      "\n",
      "  [[ -3.8184917    1.5327872   -0.9156568 ]]]\n",
      "\n",
      "\n",
      " [[[ -1.0705962    1.3645588   -2.2302496 ]]\n",
      "\n",
      "  [[  0.9711383   -2.6879628   -2.1285567 ]]\n",
      "\n",
      "  [[ -5.15031     -1.7857913   -0.64766765]]]\n",
      "\n",
      "\n",
      " [[[ -4.300858    -0.74519587   4.707138  ]]\n",
      "\n",
      "  [[  1.1525508   -1.9355469    1.1351813 ]]\n",
      "\n",
      "  [[ -1.930467     5.30831     -0.11006889]]]\n",
      "\n",
      "\n",
      " [[[ -0.30049637  -3.3917482   -0.98812234]]\n",
      "\n",
      "  [[ -0.78466344  -3.508609     1.8363969 ]]\n",
      "\n",
      "  [[  1.6145957    0.15216915  -0.27968606]]]\n",
      "\n",
      "\n",
      " [[[  1.201814     2.2275777    3.4975147 ]]\n",
      "\n",
      "  [[  1.7633957    3.9830918   10.16128   ]]\n",
      "\n",
      "  [[  1.9025049    4.217062     3.2219505 ]]]\n",
      "\n",
      "\n",
      " [[[  2.7462778   -0.87272054 -10.7139845 ]]\n",
      "\n",
      "  [[  0.5596598   -7.9665465   -3.5733411 ]]\n",
      "\n",
      "  [[  0.02203573   1.8229557    1.1090751 ]]]\n",
      "\n",
      "\n",
      " [[[ -2.8659163   -6.198704     1.1388084 ]]\n",
      "\n",
      "  [[ -5.6100855    2.2285914    1.380748  ]]\n",
      "\n",
      "  [[ -0.3560774    7.3229613    1.1240004 ]]]] \n",
      "\n",
      "(10, 3, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#                 [batch, in_height, in_width, in_channels]\n",
    "input = tf.Variable(tf.random_normal([10,5,5,1]))\n",
    "#                 [filter_height, filter_width, in_channels, out_channels]\n",
    "filter = tf.Variable(tf.random_normal([3,5,1,3]))\n",
    "op1 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(op1),'\\n')\n",
    "    print(sess.run(op1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.nn.max_pool\n",
    "[TF-池化函数 tf.nn.max_pool 的介绍](https://www.cnblogs.com/qggg/p/6832705.html)\n",
    "\n",
    "````python\n",
    "tf.nn.max_pool(value, # [batch, in_height, in_width, in_channels]\n",
    "                ksize, #  [1,in_height,in_width,1]\n",
    "                strides, # [1,height,width,1]\n",
    "                padding, # SAME VALID\n",
    "                data_format='NHWC',\n",
    "                name=None,)\n",
    "````\n",
    "\n",
    "- value: 池化层的输入，一般池化层在卷积层后面，输入通常是feature_map\n",
    "        依然是 [batch, in_height, in_width, in_channels]\n",
    "- ksize：池化窗口大小，4-D向量，一般是 [1,in_height,in_width,1]\n",
    "         因为我们不想在 batch，in_channels上做池化，所以维度为1\n",
    "- strides：步长 [1,height,width,1] 1-D向量，长度为4\n",
    "- padding：填充 SAME VALID\n",
    "\n",
    "返回一个Tensor，类型不变，shape仍然是 [batch, in_height, in_width, in_channels] 这种形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4x4图像，2通道，池化核2x2，步长1\n",
    "\n",
    "卷积由 \n",
    "[batch, in_height, in_width, in_channels]\n",
    "\n",
    "[1,4,4,2]  ->  [1,3,3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "池化前原始，image：\n",
      "\n",
      "[[[1. 2. 3. 4.]\n",
      "  [5. 6. 7. 8.]\n",
      "  [8. 7. 6. 5.]\n",
      "  [4. 3. 2. 1.]]\n",
      "\n",
      " [[4. 3. 2. 1.]\n",
      "  [8. 7. 6. 5.]\n",
      "  [1. 2. 3. 4.]\n",
      "  [5. 6. 7. 8.]]] \n",
      "\n",
      "(2, 4, 4) \n",
      "\n",
      "池化前，reshape，image：\n",
      "\n",
      "[[[[1. 2.]\n",
      "   [3. 4.]\n",
      "   [5. 6.]\n",
      "   [7. 8.]]\n",
      "\n",
      "  [[8. 7.]\n",
      "   [6. 5.]\n",
      "   [4. 3.]\n",
      "   [2. 1.]]\n",
      "\n",
      "  [[4. 3.]\n",
      "   [2. 1.]\n",
      "   [8. 7.]\n",
      "   [6. 5.]]\n",
      "\n",
      "  [[1. 2.]\n",
      "   [3. 4.]\n",
      "   [5. 6.]\n",
      "   [7. 8.]]]] \n",
      "\n",
      "(1, 4, 4, 2) \n",
      "\n",
      "池化后image：\n",
      "\n",
      "[[[[8. 7.]\n",
      "   [6. 6.]\n",
      "   [7. 8.]]\n",
      "\n",
      "  [[8. 7.]\n",
      "   [8. 7.]\n",
      "   [8. 7.]]\n",
      "\n",
      "  [[4. 4.]\n",
      "   [8. 7.]\n",
      "   [8. 8.]]]] \n",
      "\n",
      "(1, 3, 3, 2) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2x4x4\n",
    "a=tf.constant([  \n",
    "        [[1.0,2.0,3.0,4.0],  \n",
    "        [5.0,6.0,7.0,8.0],  \n",
    "        [8.0,7.0,6.0,5.0],  \n",
    "        [4.0,3.0,2.0,1.0]], # 通道1图像\n",
    "    \n",
    "        [[4.0,3.0,2.0,1.0],  # 通道2图像\n",
    "         [8.0,7.0,6.0,5.0],  \n",
    "         [1.0,2.0,3.0,4.0],  \n",
    "         [5.0,6.0,7.0,8.0]]  \n",
    "    ])  \n",
    "\n",
    "b=tf.reshape(a,[1,4,4,2])\n",
    "\n",
    "max_pool_2x2 = tf.nn.max_pool(b,[1,2,2,1],[1,1,1,1],padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('池化前原始，image：\\n')\n",
    "    print(sess.run(a),'\\n')\n",
    "    print(sess.run(a).shape,'\\n')\n",
    "    print('池化前，reshape，image：\\n')\n",
    "    print(sess.run(b),'\\n')\n",
    "    print(sess.run(b).shape,'\\n')\n",
    "    print('池化后image：\\n')\n",
    "    print(sess.run(max_pool_2x2),'\\n')\n",
    "    print(sess.run(max_pool_2x2).shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4x4图像，2通道，池化核2x2，步长2\n",
    "\n",
    "卷积由 \n",
    "[batch, in_height, in_width, in_channels]\n",
    "\n",
    "[1,4,4,2]  ->  [1,2,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "池化前原始，image：\n",
      "\n",
      "[[[1. 2. 3. 4.]\n",
      "  [5. 6. 7. 8.]\n",
      "  [8. 7. 6. 5.]\n",
      "  [4. 3. 2. 1.]]\n",
      "\n",
      " [[4. 3. 2. 1.]\n",
      "  [8. 7. 6. 5.]\n",
      "  [1. 2. 3. 4.]\n",
      "  [5. 6. 7. 8.]]] \n",
      "\n",
      "(2, 4, 4) \n",
      "\n",
      "池化前，reshape，image：\n",
      "\n",
      "[[[[1. 2.]\n",
      "   [3. 4.]\n",
      "   [5. 6.]\n",
      "   [7. 8.]]\n",
      "\n",
      "  [[8. 7.]\n",
      "   [6. 5.]\n",
      "   [4. 3.]\n",
      "   [2. 1.]]\n",
      "\n",
      "  [[4. 3.]\n",
      "   [2. 1.]\n",
      "   [8. 7.]\n",
      "   [6. 5.]]\n",
      "\n",
      "  [[1. 2.]\n",
      "   [3. 4.]\n",
      "   [5. 6.]\n",
      "   [7. 8.]]]] \n",
      "\n",
      "(1, 4, 4, 2) \n",
      "\n",
      "池化后image：\n",
      "\n",
      "[[[[8. 7.]\n",
      "   [7. 8.]]\n",
      "\n",
      "  [[4. 4.]\n",
      "   [8. 8.]]]] \n",
      "\n",
      "(1, 2, 2, 2) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2x4x4\n",
    "a=tf.constant([  \n",
    "        [[1.0,2.0,3.0,4.0],  \n",
    "        [5.0,6.0,7.0,8.0],  \n",
    "        [8.0,7.0,6.0,5.0],  \n",
    "        [4.0,3.0,2.0,1.0]], # 通道1图像\n",
    "    \n",
    "        [[4.0,3.0,2.0,1.0],  # 通道2图像\n",
    "         [8.0,7.0,6.0,5.0],  \n",
    "         [1.0,2.0,3.0,4.0],  \n",
    "         [5.0,6.0,7.0,8.0]]  \n",
    "    ])  \n",
    "\n",
    "b=tf.reshape(a,[1,4,4,2])\n",
    "\n",
    "max_pool_2x2 = tf.nn.max_pool(b,[1,2,2,1],[1,2,2,1],padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('池化前原始，image：\\n')\n",
    "    print(sess.run(a),'\\n')\n",
    "    print(sess.run(a).shape,'\\n')\n",
    "    print('池化前，reshape，image：\\n')\n",
    "    print(sess.run(b),'\\n')\n",
    "    print(sess.run(b).shape,'\\n')\n",
    "    print('池化后image：\\n')\n",
    "    print(sess.run(max_pool_2x2),'\\n')\n",
    "    print(sess.run(max_pool_2x2).shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.nn.dropout\n",
    "[TF-tf.nn.dropout介绍](https://www.cnblogs.com/qggg/p/6849881.html)\n",
    "````python\n",
    "tf.nn.dropout(\n",
    "            x, # 输入tensor\n",
    "            keep_prob=None, #每个元素保留下来的概率\n",
    "            # 一维int32张量，代表随机产生保留/丢弃表示的shape\n",
    "            noise_shape=None, \n",
    "            seed=None, # 随机种子\n",
    "            name=None, # name\n",
    "            rate=None,\n",
    ")\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据：\n",
      "\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] \n",
      "\n",
      "(10, 10) \n",
      "\n",
      "dropout：\n",
      "\n",
      "[[1.25 1.25 0.   1.25 1.25 1.25 1.25 1.25 1.25 0.  ]\n",
      " [1.25 1.25 1.25 0.   1.25 1.25 1.25 1.25 1.25 1.25]\n",
      " [1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 0.  ]\n",
      " [1.25 1.25 1.25 1.25 1.25 1.25 1.25 0.   1.25 1.25]\n",
      " [1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25]\n",
      " [1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25]\n",
      " [1.25 0.   1.25 1.25 1.25 1.25 0.   0.   1.25 1.25]\n",
      " [1.25 0.   1.25 0.   1.25 1.25 1.25 1.25 0.   1.25]\n",
      " [1.25 1.25 1.25 1.25 0.   1.25 0.   1.25 0.   1.25]\n",
      " [1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.25]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = tf.Variable(tf.ones([10,10]))\n",
    "inputs = tf.nn.dropout(x, 0.8)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('原始数据：\\n')\n",
    "    print(sess.run(x),'\\n')\n",
    "    print(sess.run(x).shape,'\\n')\n",
    "    print('dropout：\\n')\n",
    "    print(sess.run(inputs),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.reshape\n",
    "````python\n",
    "tf.reshape(tensor, shape, name=None)\n",
    "# shape里最多有一个维度的值可以填写为-1，表示自动计算此维度。\n",
    "````\n",
    "根据shape为[5,8]的tensor，生成一个新的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------case 1, 2, 5, 4---------\n",
      "[[[[1 2 3 4]\n",
      "   [5 6 7 8]\n",
      "   [7 6 5 4]\n",
      "   [3 2 1 0]\n",
      "   [3 3 3 3]]\n",
      "\n",
      "  [[3 3 3 3]\n",
      "   [1 1 1 1]\n",
      "   [1 1 1 1]\n",
      "   [2 2 2 2]\n",
      "   [2 2 2 2]]]]\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--------case -1, 2, 5, 4---------\n",
      "[[[[1 2 3 4]\n",
      "   [5 6 7 8]\n",
      "   [7 6 5 4]\n",
      "   [3 2 1 0]\n",
      "   [3 3 3 3]]\n",
      "\n",
      "  [[3 3 3 3]\n",
      "   [1 1 1 1]\n",
      "   [1 1 1 1]\n",
      "   [2 2 2 2]\n",
      "   [2 2 2 2]]]]\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--------case 8, 5, 1, 1---------\n",
      "[[[[1]]\n",
      "\n",
      "  [[2]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[4]]\n",
      "\n",
      "  [[5]]]\n",
      "\n",
      "\n",
      " [[[6]]\n",
      "\n",
      "  [[7]]\n",
      "\n",
      "  [[8]]\n",
      "\n",
      "  [[7]]\n",
      "\n",
      "  [[6]]]\n",
      "\n",
      "\n",
      " [[[5]]\n",
      "\n",
      "  [[4]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[2]]\n",
      "\n",
      "  [[1]]]\n",
      "\n",
      "\n",
      " [[[0]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[3]]]\n",
      "\n",
      "\n",
      " [[[3]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[3]]\n",
      "\n",
      "  [[1]]]\n",
      "\n",
      "\n",
      " [[[1]]\n",
      "\n",
      "  [[1]]\n",
      "\n",
      "  [[1]]\n",
      "\n",
      "  [[1]]\n",
      "\n",
      "  [[1]]]\n",
      "\n",
      "\n",
      " [[[1]]\n",
      "\n",
      "  [[1]]\n",
      "\n",
      "  [[2]]\n",
      "\n",
      "  [[2]]\n",
      "\n",
      "  [[2]]]\n",
      "\n",
      "\n",
      " [[[2]]\n",
      "\n",
      "  [[2]]\n",
      "\n",
      "  [[2]]\n",
      "\n",
      "  [[2]]\n",
      "\n",
      "  [[2]]]]\n",
      "--------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alist = [[1, 2, 3, 4, 5, 6 ,7, 8],\n",
    "         [7, 6 ,5 ,4 ,3 ,2, 1, 0],\n",
    "         [3, 3, 3, 3, 3, 3, 3, 3],\n",
    "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "         [2, 2, 2, 2, 2, 2, 2, 2]]\n",
    "oriarray = tf.constant(alist)\n",
    "oplist = []\n",
    "a1 = tf.reshape(oriarray, [1, 2, 5, 4])\n",
    "oplist.append([a1, 'case 1, 2, 5, 4'])\n",
    "\n",
    "a1 = tf.reshape(oriarray, [-1, 2, 5, 4])\n",
    "oplist.append([a1, 'case -1, 2, 5, 4'])\n",
    "\n",
    "a1 = tf.reshape(oriarray, [8, 5, 1, 1])\n",
    "oplist.append([a1, 'case 8, 5, 1, 1'])\n",
    "\n",
    "with tf.Session() as asess:\n",
    "    for aop in oplist:\n",
    "        print('--------{}---------'.format(aop[1]))\n",
    "        print(asess.run(aop[0]))\n",
    "        print('--------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-variable_scope和name_scope区别\n",
    "1、无论是variable_scope还是name_scope都对ops和tf.Variable生效。\n",
    "\n",
    "2、只有tf.get_variable在name_scope下是不生效的。\n",
    "\n",
    "3、这玩意真不好记，还是把这篇东西贴出来，以后忘记了过来看看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.name=variable_scope1/variable_scope2/v1:0,v1.op.name=variable_scope1/variable_scope2/v1\n",
      "v2.name=variable_scope1/variable_scope2/v1:0,v2.op.name=variable_scope1/variable_scope2/v1\n",
      "x1.name=variable_scope1/variable_scope2/add:0,x1.op.name=variable_scope1/variable_scope2/add\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('variable_scope1'):\n",
    "    with tf.variable_scope('variable_scope2'):\n",
    "        v1 = tf.get_variable('v1', [1])\n",
    "        v2 = tf.Variable(0,name='v2')\n",
    "        x1 = 1.0 + v1\n",
    "print('v1.name={},v1.op.name={}'.format(v1.name, v1.op.name))\n",
    "print('v2.name={},v2.op.name={}'.format(v1.name, v1.op.name))\n",
    "print('x1.name={},x1.op.name={}'.format(x1.name, x1.op.name))\n",
    "\n",
    "print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v11.name=v11:0,v11.op.name=v11\n",
      "v12.name=name_scope1/name_scope2/v12:0,v12.op.name=name_scope1/name_scope2/v12\n",
      "x11.name=name_scope1/name_scope2/add:0,x11.op.name=name_scope1/name_scope2/add\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('name_scope1'):\n",
    "    with tf.name_scope('name_scope2'):\n",
    "        v11 = tf.get_variable('v11', [1])\n",
    "        v12 = tf.Variable(0,name='v12')\n",
    "        x11 = 1.0 + v11\n",
    "print('v11.name={},v11.op.name={}'.format(v11.name, v11.op.name))\n",
    "print('v12.name={},v12.op.name={}'.format(v12.name, v12.op.name))\n",
    "print('x11.name={},x11.op.name={}'.format(x11.name, x11.op.name))\n",
    "\n",
    "print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v21.name=variable_scope3/v21:0,v21.op.name=variable_scope3/v21\n",
      "v22.name=variable_scope3/name_scope3/v22:0,v22.op.name=variable_scope3/name_scope3/v22\n",
      "x21.name=variable_scope3/name_scope3/add:0,x21.op.name=variable_scope3/name_scope3/add\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"variable_scope3\"):\n",
    "    with tf.name_scope(\"name_scope3\"):\n",
    "        v21 = tf.get_variable('v21', [1])\n",
    "        v22 = tf.Variable(0,name='v22')\n",
    "        x21 = 1.0 + v21\n",
    "print('v21.name={},v21.op.name={}'.format(v21.name, v21.op.name))\n",
    "print('v22.name={},v22.op.name={}'.format(v22.name, v22.op.name))\n",
    "print('x21.name={},x21.op.name={}'.format(x21.name, x21.op.name))\n",
    "print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1006
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4668946,
     "status": "ok",
     "timestamp": 1556373491299,
     "user": {
      "displayName": "蒋志碧",
      "photoUrl": "",
      "userId": "16736078411304620374"
     },
     "user_tz": -480
    },
    "id": "zSDiYULczBHN",
    "outputId": "8de6918c-cdaa-4e61-cac9-23581c8c011f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-bf66aaa6aa02>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-bf66aaa6aa02>:69: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-bf66aaa6aa02>:80: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Iter: 0, acc: 0.9796\n",
      "Iter: 1, acc: 0.9863\n",
      "Iter: 2, acc: 0.9882\n",
      "Iter: 3, acc: 0.9908\n",
      "Iter: 4, acc: 0.99\n",
      "Iter: 5, acc: 0.991\n",
      "Iter: 6, acc: 0.9926\n",
      "Iter: 7, acc: 0.989\n",
      "Iter: 8, acc: 0.9913\n",
      "Iter: 9, acc: 0.9896\n",
      "Iter: 10, acc: 0.992\n",
      "Iter: 11, acc: 0.9913\n",
      "Iter: 12, acc: 0.9917\n",
      "Iter: 13, acc: 0.9911\n",
      "Iter: 14, acc: 0.9912\n",
      "Iter: 15, acc: 0.9915\n",
      "Iter: 16, acc: 0.9907\n",
      "Iter: 17, acc: 0.9905\n",
      "Iter: 18, acc: 0.9923\n",
      "Iter: 19, acc: 0.9905\n",
      "Iter: 20, acc: 0.9909\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "\n",
    "def bias_vairable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# x input  [batch,in_height,in_width,in_channels]\n",
    "# [?,784] reshape -> [-1,28,28,1]\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "#w        [filter_height,filter_width,in_channels, out_channels]\n",
    "W_conv1 = weight_variable([5,5,1,32]) # 5*5的采样窗口，32个卷积核从1个平面抽取特征\n",
    "b_conv1 = bias_vairable([32]) #每个卷积核一个偏置值\n",
    "\n",
    "# 28*28*1 的图片卷积之后变为 -1*28*28*32\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "# 池化之后变为 -1*14*14*32\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 第二次卷积之后变为 14*14*64\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_vairable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "\n",
    "# 第二次池化之后变为 7*7*64\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "# 第一个全连接层\n",
    "W_fc1 = weight_variable([7*7*64,1024]) #上一层有7*7*64个神经元，全连接层有1024个神经元\n",
    "b_fc1 = bias_vairable([1024])#1024个节点\n",
    "\n",
    "# 把池化层2的输出扁平化为1维向量\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "#求第一个全连接的输出\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# 初始化第二个全连接层\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_vairable([10])\n",
    "logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "\n",
    "#计算输出\n",
    "prediction = tf.nn.sigmoid(logits)\n",
    "\n",
    "#交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "#AdamOptimizer\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "prediction_2 = tf.nn.softmax(prediction)\n",
    "correct_prediction = (tf.equal(tf.argmax(prediction_2,1), tf.argmax(y,1)))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
    "        acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})\n",
    "        print(\"Iter: \" + str(epoch) + \", acc: \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZVW2CPQzBHS"
   },
   "source": [
    "## 重构代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4371565,
     "status": "ok",
     "timestamp": 1556373491587,
     "user": {
      "displayName": "蒋志碧",
      "photoUrl": "",
      "userId": "16736078411304620374"
     },
     "user_tz": -480
    },
    "id": "o1JKG6cczBHT",
    "outputId": "60c3dd72-4498-4647-b3da-601b61d056d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1717
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5660922,
     "status": "ok",
     "timestamp": 1556374782271,
     "user": {
      "displayName": "蒋志碧",
      "photoUrl": "",
      "userId": "16736078411304620374"
     },
     "user_tz": -480
    },
    "id": "cQhT-lXizBHX",
    "outputId": "b44ead0f-74e4-4f5d-915b-571034152d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, acc: 0.11\n",
      "Step: 10, acc: 0.138\n",
      "Step: 20, acc: 0.1994\n",
      "Step: 30, acc: 0.3193\n",
      "Step: 40, acc: 0.2373\n",
      "Step: 50, acc: 0.2421\n",
      "Step: 60, acc: 0.527\n",
      "Step: 70, acc: 0.7946\n",
      "Step: 80, acc: 0.7877\n",
      "Step: 90, acc: 0.8439\n",
      "Step: 100, acc: 0.8235\n",
      "Step: 110, acc: 0.8555\n",
      "Step: 120, acc: 0.8639\n",
      "Step: 130, acc: 0.8655\n",
      "Step: 140, acc: 0.8672\n",
      "Step: 150, acc: 0.8711\n",
      "Step: 160, acc: 0.8726\n",
      "Step: 170, acc: 0.8754\n",
      "Step: 180, acc: 0.8746\n",
      "Step: 190, acc: 0.8771\n",
      "Step: 200, acc: 0.8782\n",
      "Step: 210, acc: 0.8805\n",
      "Step: 220, acc: 0.8806\n",
      "Step: 230, acc: 0.8763\n",
      "Step: 240, acc: 0.8815\n",
      "Step: 250, acc: 0.8805\n",
      "Step: 260, acc: 0.877\n",
      "Step: 270, acc: 0.8811\n",
      "Step: 280, acc: 0.8824\n",
      "Step: 290, acc: 0.8827\n",
      "Step: 300, acc: 0.8829\n",
      "Step: 310, acc: 0.8843\n",
      "Step: 320, acc: 0.8853\n",
      "Step: 330, acc: 0.8838\n",
      "Step: 340, acc: 0.8837\n",
      "Step: 350, acc: 0.8833\n",
      "Step: 360, acc: 0.8853\n",
      "Step: 370, acc: 0.885\n",
      "Step: 380, acc: 0.8841\n",
      "Step: 390, acc: 0.8871\n",
      "Step: 400, acc: 0.8871\n",
      "Step: 410, acc: 0.8877\n",
      "Step: 420, acc: 0.8874\n",
      "Step: 430, acc: 0.8888\n",
      "Step: 440, acc: 0.8869\n",
      "Step: 450, acc: 0.8879\n",
      "Step: 460, acc: 0.8876\n",
      "Step: 470, acc: 0.8894\n",
      "Step: 480, acc: 0.8898\n",
      "Step: 490, acc: 0.8901\n",
      "Step: 500, acc: 0.8893\n",
      "Step: 510, acc: 0.8892\n",
      "Step: 520, acc: 0.8868\n",
      "Step: 530, acc: 0.8862\n",
      "Step: 540, acc: 0.8881\n",
      "Step: 550, acc: 0.8863\n",
      "Step: 560, acc: 0.8881\n",
      "Step: 570, acc: 0.8894\n",
      "Step: 580, acc: 0.8902\n",
      "Step: 590, acc: 0.8897\n",
      "Step: 600, acc: 0.89\n",
      "Step: 610, acc: 0.8886\n",
      "Step: 620, acc: 0.8895\n",
      "Step: 630, acc: 0.89\n",
      "Step: 640, acc: 0.8888\n",
      "Step: 650, acc: 0.8893\n",
      "Step: 660, acc: 0.8921\n",
      "Step: 670, acc: 0.8913\n",
      "Step: 680, acc: 0.8931\n",
      "Step: 690, acc: 0.8909\n",
      "Step: 700, acc: 0.8874\n",
      "Step: 710, acc: 0.8895\n",
      "Step: 720, acc: 0.8908\n",
      "Step: 730, acc: 0.891\n",
      "Step: 740, acc: 0.8902\n",
      "Step: 750, acc: 0.8888\n",
      "Step: 760, acc: 0.8911\n",
      "Step: 770, acc: 0.8912\n",
      "Step: 780, acc: 0.889\n",
      "Step: 790, acc: 0.8929\n",
      "Step: 800, acc: 0.8939\n",
      "Step: 810, acc: 0.8917\n",
      "Step: 820, acc: 0.8891\n",
      "Step: 830, acc: 0.8923\n",
      "Step: 840, acc: 0.8947\n",
      "Step: 850, acc: 0.896\n",
      "Step: 860, acc: 0.8949\n",
      "Step: 870, acc: 0.8906\n",
      "Step: 880, acc: 0.8935\n",
      "Step: 890, acc: 0.8918\n",
      "Step: 900, acc: 0.892\n",
      "Step: 910, acc: 0.8919\n",
      "Step: 920, acc: 0.8919\n",
      "Step: 930, acc: 0.8938\n",
      "Step: 940, acc: 0.8928\n",
      "Step: 950, acc: 0.8941\n",
      "Step: 960, acc: 0.8925\n",
      "Step: 970, acc: 0.891\n",
      "Step: 980, acc: 0.8947\n",
      "Step: 990, acc: 0.8984\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "max_step = 1000\n",
    "keep_ = 0.8\n",
    "log_dir = \"Logs/log-6.1\"\n",
    "\n",
    "# 生成权重\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape,stddev=0.1),name='W')\n",
    "\n",
    "# 生成偏差\n",
    "def bias_vairable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape),name='b')\n",
    "\n",
    "# 记录变量\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME',name='conv2d')\n",
    "        \n",
    "def conv_layer(input_tensor, weight_shape, layer_name, act=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable(weight_shape)\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_vairable([weight_shape[-1]])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('conv_comput'):\n",
    "            preactivate = conv2d(input_tensor,weights) + biases\n",
    "        with tf.name_scope('activate'):\n",
    "            activations = act(preactivate)\n",
    "        return activations\n",
    "\n",
    "def linear_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_vairable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('linear_comput'):\n",
    "            preactivate = tf.matmul(input_tensor,weights) + biases\n",
    "        with tf.name_scope('activate'):\n",
    "            activations = act(preactivate)\n",
    "        return activations\n",
    "        \n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name='Max_pool')\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32,[None,784],name='input_x')\n",
    "    with tf.name_scope('Input_reshape'):\n",
    "        x_image = tf.reshape(x,[-1,28,28,1],name='x-image')\n",
    "        tf.summary.image('input',x_image,10)\n",
    "    y = tf.placeholder(tf.float32,[None,10],name='input_y')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "# 第一次卷积   28*28*1->28*28*32\n",
    "conv_layer1 = conv_layer(x_image,[5,5,1,32],'conv_layer1')\n",
    "# 池化之后变为 14*14*32\n",
    "with tf.name_scope('Max_pool1'):\n",
    "    h_pool1 = max_pool_2x2(conv_layer1)\n",
    "\n",
    "# 第二次卷积 14*14*32->14*14*64\n",
    "conv_layer2 = conv_layer(h_pool1,[5,5,32,64],'conv_layer2')\n",
    "# 第二次池化之后变为 7*7*64\n",
    "with tf.name_scope('Max_pool2'):\n",
    "    h_pool2 = max_pool_2x2(conv_layer2)\n",
    "\n",
    "with tf.name_scope('Flatten'):\n",
    "    flatten_ = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "    \n",
    "# 第一个全连接层 7*7*64 - 1024\n",
    "fc1 = linear_layer(flatten_, 7*7*64, 1024, 'FC1')\n",
    "\n",
    "with tf.name_scope('Dropput'):\n",
    "    fc1_drop = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "# 第二个全连接层 1024 - 10\n",
    "logits = linear_layer(fc1_drop, 1024, 10, 'FC2',act=tf.nn.sigmoid)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "def get_dict(train):\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        k = keep_\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return {x:xs, y:ys, keep_prob: k}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(max_step):\n",
    "        if i%10 == 0:\n",
    "            summary,acc = sess.run([merged,accuracy], feed_dict=get_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print(\"Step: \" + str(i) + \", acc: \" + str(acc))\n",
    "        else:\n",
    "            summary,_ = sess.run([merged,train_step], feed_dict=get_dict(True))\n",
    "            train_writer.add_summary(summary,i)\n",
    "        \n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6.1_卷积神经网络.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
