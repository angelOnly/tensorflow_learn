{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于tensorflow的NN，tensor表示数据，graph搭建神经网络，session执行graph，优化线上的权重(参数)，得到模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量(tensor)：多维数组(列表)，阶：张量的维度\n",
    "张量表示 0阶到n阶的数组\n",
    "\n",
    "| 维数 | 阶   | 名字        | 例子                        |\n",
    "| ---- | ---- | ----------- | --------------------------- |\n",
    "| 0-D  | 0    | 标量 scalar | s = 123                     |\n",
    "| 1-D  | 1    | 向量 vector | v=[1,2,3]                   |\n",
    "| 2-D  | 2    | 矩阵 matrix | m=[[1,2,3],[4,5,6],[7,8,9]] |\n",
    "| n-D  | n    | 张量 tensor | t=[[[[.....]]]] n个         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据类型: tf.float32  tf.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([1.0,2.0])\n",
    "b = tf.constant([3.0,4.0])\n",
    "\n",
    "result = a+b\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tensor(\"add:0\",         shape=(2,),         dtype=float32)\n",
    "\n",
    "        节点名：第0个输出，维度，一维数组，长度2  数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会话 session\n",
    "执行图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 6.]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图\n",
    "y = wx+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0,2.0]])\n",
    "w= tf.constant([[3.0],[4.0]])\n",
    "y = tf.matmul(x,w)\n",
    "print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播\n",
    "![image.png](./img/params.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w = tf.Variable(tf.random_normal([2,3],stddev=2,mean=0, seed=1))\n",
    "                   正态分布      2x3矩阵  标准差2  均值0    随机种子\n",
    "\n",
    "    tf.truncated_normal()：去掉过大偏离点的正态分布\n",
    "    tf.random_uniform()：平均分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的实现\n",
    "1. 准备数据集，提取特征，作为输入喂给神经网络\n",
    "2. 搭建NN结构，从输入到输出(先搭建计算图，再用回话执行) NN前向传播算法 -> 计算输出\n",
    "3. 大量特征数据喂给NN，迭代优化NN参数  NN反向传播算法 -> 优化参数训练模型\n",
    "4. 使用训练好的模型预测和分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0.7 0.5]] (1, 2) \n",
      "\n",
      "w1: [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]] ,w1 shape (2, 3) \n",
      "\n",
      "w2: [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]] ,w2 shape (3, 1) \n",
      "\n",
      "a: [[-1.7892749   1.0888433   0.34134272]] , a shape: (1, 3) \n",
      "\n",
      "y: [[3.0904665]] , y shape: (1, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[0.7,0.5]])\n",
    "w1 = tf.Variable(tf.random_normal([2,3], stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1], stddev=1,seed=1))\n",
    "\n",
    "#定义前向传播过程\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#用会话计算结果\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('x:',sess.run(x),sess.run(x).shape,'\\n')\n",
    "    print('w1:',sess.run(w1),',w1 shape',sess.run(w1).shape,'\\n')\n",
    "    print('w2:',sess.run(w2),',w2 shape',sess.run(w2).shape,'\\n')\n",
    "    print('a:',sess.run(a),', a shape:',sess.run(a).shape,'\\n')\n",
    "    print('y:',sess.run(y),', y shape:',sess.run(y).shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 两层简单的神经网络，全连接\n",
    "##### 用placeholder定义输入，sess.run()喂入一组数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: [[3.0904665]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用placeholder定义输入，sess.run()喂入一组数据\n",
    "x = tf.placeholder(tf.float32, shape=(1,2))\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "#定义前向传播过程\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#用会话计算结果\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('y:',sess.run(y, feed_dict={x:[[0.7,0.5]]}),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用placeholder定义输入，sess.run()喂入多组数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: [[3.0904665]\n",
      " [1.2236414]\n",
      " [1.7270732]\n",
      " [2.2305048]] \n",
      "\n",
      "w1: [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]] ,w1 shape (2, 3) \n",
      "\n",
      "w2: [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]] ,w2 shape (3, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用placeholder定义输入，sess.run()喂入一组数据\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "#定义前向传播过程\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#用会话计算结果\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('y:',sess.run(y, feed_dict={x:[[0.7,0.5],[0.2,0.3],[0.3,0.4],[0.4,0.5]]}),'\\n')\n",
    "    print('w1:',sess.run(w1),',w1 shape',sess.run(w1).shape,'\\n')\n",
    "    print('w2:',sess.run(w2),',w2 shape',sess.run(w2).shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播\n",
    "训练模型参数，在所有参数上用梯度下降，使NN模型在训练数据上的损失最小\n",
    "\n",
    "**均方误差MSE** loss = tf.reduce_mean(tf.square(y-y_predict))\n",
    "\n",
    "**交叉熵**   loss = tf.reduce_mean(loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.argmax(y_,1),logits=y)))\n",
    "\n",
    "**反向传播训练方法，以减小liss值为优化目标**\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "#### 示例code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未经训练的参数：\n",
      "\n",
      "w1: [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]] ,w1 shape (2, 3) \n",
      "\n",
      "w2: [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]] ,w2 shape (3, 1) \n",
      "\n",
      "训练模型...\n",
      "After 0 training step(s), loss on all data is 17.7311:\n",
      "After 500 training step(s), loss on all data is 3.64612:\n",
      "After 1000 training step(s), loss on all data is 0.844747:\n",
      "After 1500 training step(s), loss on all data is 0.696087:\n",
      "After 2000 training step(s), loss on all data is 0.693902:\n",
      "After 2500 training step(s), loss on all data is 0.693887:\n",
      "After 3000 training step(s), loss on all data is 0.693887:\n",
      "\n",
      "训练后的参数：\n",
      "\n",
      "w1: [[-0.31591475  0.92382187  1.2035394 ]\n",
      " [-2.1698406  -0.19345604  1.0057589 ]] ,w1 shape (2, 3) \n",
      "\n",
      "w2: [[-0.38026863]\n",
      " [ 0.9284347 ]\n",
      " [-0.9417927 ]] ,w2 shape (3, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "#基于seed产生随机数\n",
    "rng = np.random.RandomState(2019)\n",
    "#随机数返回(32,2)的矩阵\n",
    "X = rng.randn(32,2)\n",
    "#从X中取出1行，判断如果小于1，Y=1，如果不小于1，Y=0\n",
    "Y = [[int(x0+x1 < 1)] for (x0,x1) in X]\n",
    "\n",
    "# 定义神经网络的输入，参数，输出，定义前向传播过程\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#定义loss及反向传播方法\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#用会话计算结果\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('未经训练的参数：\\n')\n",
    "    print('w1:',sess.run(w1),',w1 shape',sess.run(w1).shape,'\\n')\n",
    "    print('w2:',sess.run(w2),',w2 shape',sess.run(w2).shape,'\\n')\n",
    "    print('训练模型...')\n",
    "    for i in range(3001):\n",
    "        start = (i*batch_size) % 32\n",
    "        end = start + batch_size\n",
    "        sess.run(train_step, feed_dict={x:X[start:end], y_:Y[start:end]})\n",
    "        if i%500==0:\n",
    "            total_loss = sess.run(loss, feed_dict={x:X, y_:Y})\n",
    "            print('After %d training step(s), loss on all data is %g:' %(i, total_loss))\n",
    "            \n",
    "    print('\\n训练后的参数：\\n')\n",
    "    print('w1:',sess.run(w1),',w1 shape',sess.run(w1).shape,'\\n')\n",
    "    print('w2:',sess.run(w2),',w2 shape',sess.run(w2).shape,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率\n",
    "\n",
    "学习率过大：震荡不收敛\n",
    "\n",
    "学习率过小：收敛速度慢\n",
    "\n",
    "**指数衰减学习率**\n",
    "\n",
    "learning_rate = learning_rate_base(学习率初始值) * learning_rate_decay(学习率衰减率) 多少轮更新一次学习率(global_step/运行了几轮的batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "global_step = tf.Variable(0, trainable=False) # 只用于计数，不可训练\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,\n",
    "                                           global_step,\n",
    "                                           LEARNING_RATE_STEP,\n",
    "                                           LEARNING_RATE_DECAY,\n",
    "                                           staircase=True) # True：阶梯式衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.exponential_decay(\n",
    "    learning_rate,\n",
    "    global_step,\n",
    "    decay_steps,\n",
    "    decay_rate,\n",
    "    staircase=False,\n",
    "    name=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练模型...\n",
      "After 0 step(s), global_step is 1.000000, w is 4.900000, learning_rate is 0.099000, loss is 34.810001\n",
      "After 1 step(s), global_step is 2.000000, w is 4.801047, learning_rate is 0.098010, loss is 33.652149\n",
      "After 2 step(s), global_step is 3.000000, w is 4.703162, learning_rate is 0.097030, loss is 32.526058\n",
      "After 3 step(s), global_step is 4.000000, w is 4.606363, learning_rate is 0.096060, loss is 31.431309\n",
      "After 4 step(s), global_step is 5.000000, w is 4.510670, learning_rate is 0.095099, loss is 30.367481\n",
      "After 5 step(s), global_step is 6.000000, w is 4.416100, learning_rate is 0.094148, loss is 29.334135\n",
      "After 6 step(s), global_step is 7.000000, w is 4.322670, learning_rate is 0.093207, loss is 28.330811\n",
      "After 7 step(s), global_step is 8.000000, w is 4.230396, learning_rate is 0.092274, loss is 27.357040\n",
      "After 8 step(s), global_step is 9.000000, w is 4.139294, learning_rate is 0.091352, loss is 26.412340\n",
      "After 9 step(s), global_step is 10.000000, w is 4.049376, learning_rate is 0.090438, loss is 25.496202\n",
      "After 10 step(s), global_step is 11.000000, w is 3.960657, learning_rate is 0.089534, loss is 24.608120\n",
      "After 11 step(s), global_step is 12.000000, w is 3.873148, learning_rate is 0.088638, loss is 23.747572\n",
      "After 12 step(s), global_step is 13.000000, w is 3.786858, learning_rate is 0.087752, loss is 22.914005\n",
      "After 13 step(s), global_step is 14.000000, w is 3.701797, learning_rate is 0.086875, loss is 22.106895\n",
      "After 14 step(s), global_step is 15.000000, w is 3.617973, learning_rate is 0.086006, loss is 21.325674\n",
      "After 15 step(s), global_step is 16.000000, w is 3.535392, learning_rate is 0.085146, loss is 20.569778\n",
      "After 16 step(s), global_step is 17.000000, w is 3.454060, learning_rate is 0.084294, loss is 19.838648\n",
      "After 17 step(s), global_step is 18.000000, w is 3.373980, learning_rate is 0.083451, loss is 19.131706\n",
      "After 18 step(s), global_step is 19.000000, w is 3.295156, learning_rate is 0.082617, loss is 18.448370\n",
      "After 19 step(s), global_step is 20.000000, w is 3.217590, learning_rate is 0.081791, loss is 17.788065\n",
      "After 20 step(s), global_step is 21.000000, w is 3.141281, learning_rate is 0.080973, loss is 17.150209\n",
      "After 21 step(s), global_step is 22.000000, w is 3.066229, learning_rate is 0.080163, loss is 16.534218\n",
      "After 22 step(s), global_step is 23.000000, w is 2.992431, learning_rate is 0.079361, loss is 15.939507\n",
      "After 23 step(s), global_step is 24.000000, w is 2.919886, learning_rate is 0.078568, loss is 15.365503\n",
      "After 24 step(s), global_step is 25.000000, w is 2.848588, learning_rate is 0.077782, loss is 14.811627\n",
      "After 25 step(s), global_step is 26.000000, w is 2.778532, learning_rate is 0.077004, loss is 14.277307\n",
      "After 26 step(s), global_step is 27.000000, w is 2.709713, learning_rate is 0.076234, loss is 13.761971\n",
      "After 27 step(s), global_step is 28.000000, w is 2.642123, learning_rate is 0.075472, loss is 13.265057\n",
      "After 28 step(s), global_step is 29.000000, w is 2.575753, learning_rate is 0.074717, loss is 12.786010\n",
      "After 29 step(s), global_step is 30.000000, w is 2.510595, learning_rate is 0.073970, loss is 12.324280\n",
      "After 30 step(s), global_step is 31.000000, w is 2.446640, learning_rate is 0.073230, loss is 11.879326\n",
      "After 31 step(s), global_step is 32.000000, w is 2.383876, learning_rate is 0.072498, loss is 11.450616\n",
      "After 32 step(s), global_step is 33.000000, w is 2.322292, learning_rate is 0.071773, loss is 11.037626\n",
      "After 33 step(s), global_step is 34.000000, w is 2.261877, learning_rate is 0.071055, loss is 10.639842\n",
      "After 34 step(s), global_step is 35.000000, w is 2.202618, learning_rate is 0.070345, loss is 10.256760\n",
      "After 35 step(s), global_step is 36.000000, w is 2.144501, learning_rate is 0.069641, loss is 9.887885\n",
      "After 36 step(s), global_step is 37.000000, w is 2.087513, learning_rate is 0.068945, loss is 9.532737\n",
      "After 37 step(s), global_step is 38.000000, w is 2.031640, learning_rate is 0.068255, loss is 9.190842\n",
      "After 38 step(s), global_step is 39.000000, w is 1.976868, learning_rate is 0.067573, loss is 8.861741\n",
      "After 39 step(s), global_step is 40.000000, w is 1.923181, learning_rate is 0.066897, loss is 8.544985\n",
      "After 40 step(s), global_step is 41.000000, w is 1.870564, learning_rate is 0.066228, loss is 8.240136\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE_BASE = 0.1 #最初学习率\n",
    "LEARNING_RATE_DECAY = 0.99 #学习率衰减因子\n",
    "LEARNING_RATE_STEP = 1 #喂入多少轮batch_size后，更新一次学习率，一般设为 global/batch_size\n",
    "\n",
    "#运行了几轮batch_size的计数器，初始值为0，设为不被训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#定义指数下降学习率\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,\n",
    "                                          global_step,\n",
    "                                          LEARNING_RATE_STEP,\n",
    "                                          LEARNING_RATE_DECAY,\n",
    "                                          staircase=True,\n",
    "                                          name='learning_rate')\n",
    "\n",
    "#定义优化参数，初始值为0\n",
    "w = tf.Variable(tf.constant(5,tf.float32), name='weights')\n",
    "#定义loss\n",
    "loss = tf.square(w+1)\n",
    "#定义反向传播方法\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#生成会话，训练\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('训练模型...')\n",
    "    for i in range(41):\n",
    "        sess.run(train_step)\n",
    "        w_val = sess.run(w)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        loss_val = sess.run(loss)\n",
    "        print('After %d step(s), global_step is %f, w is %f, learning_rate is %f, loss is %f' %(i, global_step_val,w_val,learning_rate_val,loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 滑动平均\n",
    "记录每个参数一段时间内过往值的平均，增加了模型的泛化性\n",
    "\n",
    "针对所有参数w和b，像是给参数加了影子，参数变化，影子缓慢追随\n",
    "\n",
    "衰减率 = min{MOVINg_AVERAGE_DECAY, (1+轮数) / (10+轮数)}\n",
    "\n",
    "影子 = 衰减率 * 影子 + (1-衰减率) * 参数     影子初始值 = 参数初始值\n",
    "\n",
    "MOVING_AVERAGE_DECAY=0.99, w1=0, global_step=0, w1的滑动平均为0\n",
    "\n",
    "w1更新为1时：\n",
    "- w1的滑动平均值 = min(0.99, 1/10) * 0 + (1-min(0.99, 1/10) * 1) = 0.9\n",
    "\n",
    "w1更新为10,global_step更新为100时：\n",
    "- w1的滑动平均值 = min(0.99, 101/110) * 0.9 + (1-min(0.99, 101/110) * 10) = 1.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.ExponentialMovingAverage(\n",
    "    decay,\n",
    "    num_updates=None,\n",
    "    zero_debias=False,\n",
    "    name='ExponentialMovingAverage',\n",
    ")\n",
    "\n",
    "ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "ema_op = ema.apply(tf.trainable_variables()) #每次运行此句，所有待优化参数求滑动平均\n",
    "with tf.control_dependencies([train_step, ema_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "    \n",
    "#ema.average(参数名) #查看某参数的滑动平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始w=0： [5.0, 5.0] \n",
      "\n",
      "w=1:\n",
      "[1.0, 1.4000001] \n",
      "\n",
      "w=10,global_step=100:\n",
      "[10.0, 2.1036363] \n",
      "\n",
      "[10.0, 2.7497022] \n",
      "\n",
      "[10.0, 3.3429084] \n",
      "\n",
      "[10.0, 3.8875794] \n",
      "\n",
      "[10.0, 4.3876867] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "MOVING_AVERAGE_DECAY = 0.99 #学习率衰减因子\n",
    "\n",
    "#运行了几轮batch_size的计数器，初始值为0，设为不被训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#定义优化参数，初始值为0\n",
    "w = tf.Variable(tf.constant(5,tf.float32), name='weights')\n",
    "\n",
    "#实例化滑动平均类，给删减率为0.99，当前轮数global_step\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "# ema.apply后的括号更新列表，每次运行sess.run(ema_op)时，对更新列表中的元素求滑动平均值\n",
    "# 在实际应用中会使用tf.trainable_variables()将所有训练参数汇总为列表\n",
    "# ema_op = ema.apply([w])\n",
    "ema_op = ema.apply(tf.trainable_variables())\n",
    "\n",
    "#查看不同迭代中变量取值的变化\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('初始w=0：',sess.run([w,ema.average(w)]),'\\n')\n",
    "    \n",
    "    # 参数w赋值为1\n",
    "    print('w=1:')\n",
    "    sess.run(tf.assign(w,1))\n",
    "    sess.run(ema_op)\n",
    "    print(sess.run([w,ema.average(w)]),'\\n')\n",
    "    \n",
    "    #更新step和w，模拟出100轮后，w=10\n",
    "    print('w=10,global_step=100:')\n",
    "    sess.run(tf.assign(global_step,100))\n",
    "    sess.run(tf.assign(w,10))\n",
    "    sess.run(ema_op)\n",
    "    print(sess.run([w,ema.average(w)]),'\\n')\n",
    "    \n",
    "    sess.run(ema_op)\n",
    "    print(sess.run([w,ema.average(w)]),'\\n')\n",
    "    \n",
    "    sess.run(ema_op)\n",
    "    print(sess.run([w,ema.average(w)]),'\\n')\n",
    "    \n",
    "    sess.run(ema_op)\n",
    "    print(sess.run([w,ema.average(w)]),'\\n')\n",
    "    \n",
    "    sess.run(ema_op)\n",
    "    print(sess.run([w,ema.average(w)]),'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "360px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
